# Import all the required libraries.

import numpy as np
import pandas as pd
import seaborn as sns
from numpy import argmax
import ast

import librosa
import librosa.display
import os
import os.path
from os import path
from os.path import exists

import IPython.display as ipd
import soundfile as sf

import sklearn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler

%matplotlib inline
import matplotlib.pyplot as plt

import random

from PIL import Image
import pathlib
import csv 

# Keras
import keras
from keras import layers
from keras.models import Sequential
from keras.layers import Activation, Dense, Dropout, Conv2D, Flatten, MaxPooling2D, BatchNormalization
from keras.layers import GlobalMaxPooling2D, GlobalAveragePooling1D, AveragePooling2D, Input, Add
from tensorflow.keras.optimizers import Adam
from keras import backend as K
import tensorflow as tf

# Import necessary libraries for metrics
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

from datetime import datetime

import warnings
warnings.filterwarnings('ignore')

import random

print('Libraries have been imported')

def get_all_labels(bird, filename):
    
    search_key = bird + '/' + filename
    
    primary_label = train_md.loc[train_md['filename'] == search_key]['primary_label'].tolist()
    secondary_labels = train_md.loc[train_md['filename'] == search_key]['secondary_labels'].values
    
    if len(primary_label)==0:
        primary_label = [bird]
        
    if len(secondary_labels)==0:
        secondary_labels = []
    else:
        secondary_labels = ast.literal_eval(secondary_labels[0])
    
    all_labels = primary_label + secondary_labels

    return all_labels
    
print('Functions have been defined')

# Example of Spectogram

# Spectogram

X = librosa.stft(x)
Xdb = librosa.amplitude_to_db(abs(X))
plt.figure(figsize=(14, 5))
librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')
plt.colorbar();

# Same at a logarithmic scale

librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log')
plt.colorbar();

## From audio to image spectogram

# From audio to spectogram - example
signal, sr = librosa.load('XC125458.ogg')
n_fft = 2048
# The amount of samples we are shifting after each fft
hop_length = 512
# Short-time Fourier Transformation on our audio data
audio_stft = librosa.core.stft(signal, hop_length=hop_length, n_fft=n_fft)
# gathering the absolute values for all values in our audio_stft
spectrogram = np.abs(audio_stft)
# Plotting the short-time Fourier Transformation
plt.figure(figsize=(20, 5))
# Using librosa.display.specshow() to create our spectrogram
librosa.display.specshow(spectrogram, sr=sr, x_axis='time',        y_axis='hz', hop_length=hop_length)
plt.colorbar(label='Amplitude')
plt.title('Spectrogram (amp)', fontdict=dict(size=18))
plt.xlabel('Time', fontdict=dict(size=15))
plt.ylabel('Frequency', fontdict=dict(size=15))
plt.show();

# Short-time Fourier Transformation on our audio data
audio_stft = librosa.core.stft(signal, hop_length=hop_length, n_fft=n_fft)
# gathering the absolute values for all values in our audio_stft 
spectrogram = np.abs(audio_stft)
# Converting the amplitude to decibels
log_spectro = librosa.amplitude_to_db(spectrogram)
# Plotting the short-time Fourier Transformation
plt.figure(figsize=(20, 5))
# Using librosa.display.specshow() to create our spectrogram
librosa.display.specshow(log_spectro, sr=sr, x_axis='time', y_axis='hz', hop_length=hop_length, cmap='magma')
plt.colorbar(label='Decibels')
plt.title('Spectrogram (dB)', fontdict=dict(size=18))
plt.xlabel('Time', fontdict=dict(size=15))
plt.ylabel('Frequency', fontdict=dict(size=15))
plt.show()

# Mel spectogram
signal, sr = librosa.load('XC125458.ogg')

mel_signal = librosa.feature.melspectrogram(y=signal, sr=sr, hop_length=hop_length, n_fft=n_fft)
spectrogram = np.abs(mel_signal)
power_to_db = librosa.power_to_db(spectrogram, ref=np.max)
plt.figure(figsize=(20, 5))
plt.title('Mel Spectograms', fontdict=dict(size=18))
plt.xlabel('Time', fontdict=dict(size=15))
plt.ylabel('Frequency', fontdict=dict(size=15))
librosa.display.specshow(power_to_db, sr=sr, x_axis='time', y_axis='mel', cmap='magma', hop_length=hop_length);

# Now convert the audio data files into PNG format images
# Basically extracting the Spectrogram for every Audio. 

print('Creation of mel spectograms ...\n')
now = datetime.now()
show_time = now.strftime("%H:%M:%S")
print("begin =", show_time)

# We will use librosa python library to extract Spectrogram for every audio file.

n_fft = 2048
# The amount of samples we are shifting after each fft
hop_length = 512

if not (path.exists(f'img_data')):
    pathlib.Path(f'img_data').mkdir(parents=True, exist_ok=True)

# subdirectories per species and spectograms for each species are created
for bird in birds:
    print('Creating spectograms for ', bird)
    cnt = 0
    if not (path.exists(f'img_data/{bird}')):
        pathlib.Path(f'img_data/{bird}').mkdir(parents=True, exist_ok=True)
    for filename in os.listdir(f'./train_audio/{bird}'):
        cnt += 1
        songname = f'./train_audio/{bird}/{filename}'
        y, sr = librosa.load(songname, mono=True, duration=45)
        mel_signal = librosa.feature.melspectrogram(y=y, sr=sr, hop_length=hop_length, n_fft=n_fft)
        spectrogram = np.abs(mel_signal)
        power_to_db = librosa.power_to_db(spectrogram, ref=np.max)
        plt.figure(figsize=(8, 7))
        librosa.display.specshow(power_to_db, sr=sr, x_axis='time', y_axis='mel', cmap='magma', hop_length=hop_length)
        plt.axis('off');
        plt.savefig(f'img_data/{bird}/{filename[:-3].replace(".", "")}.png');
        plt.clf()
    print(cnt, ' spectograms created for ', bird)

print('All mel spectograms created...')
now = datetime.now()
show_time = now.strftime("%H:%M:%S")
print("end =", show_time)

# We will split data by 75% in training, 10% in the test set, and 15% in the validation set.

import splitfolders

# To only split into training, test, and validation sets, set a tuple to `ratio`, i.e, `(.75, .10, .15)`.
splitfolders.ratio('./img_data/', output="./datasets", seed=1337, ratio=(.75, .10, .15)) # default values

print('Train, test, and validation folders have been created')

from keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(rescale=1./255,  # rescale all pixel values from 0-255, so after this step all our pixel values are in range (0,1)
                                   shear_range=0.2, # to apply some random tranfromations
                                   zoom_range=0.2,  # to apply zoom
                                   horizontal_flip=True # image will be flipper horizontally
                                  ) 
val_datagen = ImageDataGenerator(rescale=1./255)

test_datagen = ImageDataGenerator(rescale=1./255)

train_set = train_datagen.flow_from_directory('./datasets/train',
                                              target_size=(64, 64),
                                              batch_size=16,
                                              class_mode='categorical',
                                              shuffle = False
                                             )

val_set = val_datagen.flow_from_directory('./datasets/val',
                                            target_size=(64, 64),
                                            batch_size=1,
                                            class_mode='categorical',
                                            shuffle = False
                                           )
test_set = test_datagen.flow_from_directory('./datasets/test',
                                            target_size=(64, 64),
                                            batch_size=1,
                                            class_mode='categorical',
                                            shuffle = False
                                           )

print('Ready')

# Create a Convolutional Neural Network:

model = Sequential()

input_shape=(64, 64, 3)

#1st hidden layer
model.add(Conv2D(64, (3, 3), strides=(2, 2), input_shape=input_shape))
model.add(AveragePooling2D((2, 2), strides=(2,2), padding="same"))
model.add(Activation('relu'))
model.add(BatchNormalization())

#2nd hidden layer
model.add(Conv2D(64, (3, 3), padding="same"))
model.add(AveragePooling2D((2, 2), strides=(2,2)))
model.add(Activation('relu'))
model.add(BatchNormalization())

#3rd hidden layer
model.add(Conv2D(64, (3, 3), padding="same"))
model.add(AveragePooling2D((2, 2), strides=(2,2), padding="same"))
model.add(Activation('relu'))
model.add(BatchNormalization())

#4th hidden layer
model.add(Conv2D(64, (3, 3), padding="same"))
model.add(AveragePooling2D((2, 2), strides=(2,2), padding="same"))
model.add(Activation('relu'))
model.add(BatchNormalization())

#Flatten
model.add(Flatten())
model.add(Dropout(rate=0.5))

#Add fully connected layer.
model.add(Dense(64))
model.add(Activation('relu'))
model.add(Dropout(rate=0.5))

#Add fully connected layer.
model.add(Dense(32))
model.add(Activation('relu'))
model.add(Dropout(rate=0.5))

#Output layer
model.add(Dense(21))
model.add(Activation('softmax'))

model.summary()

#  Hyperparameters and model compilation

learning_rate = 0.0001
epochs = 50
batch_size = 8
optimizer = Adam(learning_rate=learning_rate)
model.compile(optimizer=optimizer,
              loss="categorical_crossentropy", 
              metrics=['accuracy']
             )

print('Ready')

# CNN Training

# Found 1005 images belonging to 21 classes - training
# Found 126 images belonging to 21 classes - validation
# Found 218 images belonging to 21 classes - test

# steps_per_epoch = 1074//batch_size

# validation_steps = 149//batch_size # if you have validation data 

#  Now fit the model with 100 epochs.

history = model.fit_generator(train_set,
                              steps_per_epoch=None,
                              epochs=epochs,
                              validation_data=val_set,
                              validation_steps=None
                             )

print('Ready')

# Call the predict_generator

test_set.reset()
pred = model.predict_generator(test_set, steps=50, verbose=1)

print('Ready')

# Find out what you predicted for which image.

predicted_class_indices = np.argmax(pred,axis=1)

labels = (train_set.class_indices)
labels = dict((v,k) for k,v in labels.items())

predictions = [labels[k] for k in predicted_class_indices]
predictions = predictions[:200]

filenames = test_set.filenames

print('Ready')

# Save the results to a CSV file.

results = pd.DataFrame({"Filename":filenames,
                        "Predictions":predictions},
                       # orient='index'
                      )
results.to_csv("prediction_results.csv",index=False)

print('Ready')
